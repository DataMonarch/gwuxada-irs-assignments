{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0101101101110111\n",
      "101101101110111\n",
      "01101101110111\n",
      "1101101110111\n",
      "101101110111\n",
      "01101110111\n",
      "1101110111\n",
      "101110111\n",
      "01110111\n"
     ]
    }
   ],
   "source": [
    "initial_str = \"0101101101110111\"\n",
    "\n",
    "sistring_list = []\n",
    "\n",
    "for i in range(len(initial_str)):\n",
    "    sistring = initial_str[i:]\n",
    "    sistring_list.append(sistring)\n",
    "    print(sistring)\n",
    "\n",
    "    if i == 8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sistring_list(initial_str: str, num_of_sistrings:int = 9) -> list:\n",
    "    sistring_list = []\n",
    "    for i in range(len(initial_str)):\n",
    "        sistring = initial_str[i:]\n",
    "        sistring_list.append(sistring)\n",
    "        if i == num_of_sistrings - 1:\n",
    "            break\n",
    "        \n",
    "    return sistring_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_prefixes(sistring_list: list) -> list:\n",
    "\n",
    "    # Find the longest common prefix\n",
    "    prefix = os.path.commonprefix(sistring_list)\n",
    "    unique_prefixes = []\n",
    "    # Print the unique prefix for each string\n",
    "    for string in sistring_list:\n",
    "        if string.startswith(prefix):\n",
    "            # Remove the prefix from the current string to get the unique prefix\n",
    "            unique_prefix = string.replace(prefix, '', 1)\n",
    "            unique_prefixes.append(unique_prefix)\n",
    "    \n",
    "    return unique_prefixes            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0101101101110111',\n",
       " '101101101110111',\n",
       " '01101101110111',\n",
       " '1101101110111',\n",
       " '101101110111',\n",
       " '01101110111',\n",
       " '1101110111',\n",
       " '101110111',\n",
       " '01110111']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_str = \"0101101101110111\"\n",
    "sistrings = get_sistring_list(initial_str, 9)\n",
    "sistrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0101101101110111',\n",
       " '101101101110111',\n",
       " '01101101110111',\n",
       " '1101101110111',\n",
       " '101101110111',\n",
       " '01101110111',\n",
       " '1101110111',\n",
       " '101110111',\n",
       " '01110111']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unique_prefixes(sistrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['010', '0101', '01011', '010110', '0101101', '01011011', '010110110', '0101101101', '01011011011', '010110110111', '0101101101110', '01011011011101', '010110110111011', '0101101101110111', '10110110', '101101101', '1011011011', '10110110111', '101101101110', '1011011011101', '10110110111011', '101101101110111', '0110110', '01101101', '011011011', '0110110111', '01101101110', '011011011101', '0110110111011', '01101101110111', '110110', '1101101', '11011011', '110110111', '1101101110', '11011011101', '110110111011', '1101101110111', '10110111', '101101110', '1011011101', '10110111011', '101101110111', '0110111', '01101110', '011011101', '0110111011', '01101110111', '110111', '1101110', '11011101', '110111011', '1101110111', '10111', '101110', '1011101', '10111011', '101110111', '0111', '01110', '011101', '0111011', '01110111']\n"
     ]
    }
   ],
   "source": [
    "from pygtrie import Trie\n",
    "\n",
    "def unique_prefixes(strings):\n",
    "    trie = Trie()\n",
    "    prefixes = {}\n",
    "    for s in strings:\n",
    "        for i in range(len(s)):\n",
    "            prefix = s[:i+1]\n",
    "            if prefix in prefixes:\n",
    "                del prefixes[prefix]\n",
    "            elif prefix not in trie:\n",
    "                trie[prefix] = True\n",
    "                prefixes[prefix] = s\n",
    "    return list(prefixes.keys())\n",
    "\n",
    "print(unique_prefixes(sistrings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010\n",
      "10110110\n",
      "0110110\n",
      "110110\n",
      "10110111\n",
      "0110111\n",
      "110111\n",
      "10111\n",
      "0111\n"
     ]
    }
   ],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.prefix_count = 0\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "            node.prefix_count += 1\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def find_unique_prefix(self, word):\n",
    "        node = self.root\n",
    "        prefix = \"\"\n",
    "        for char in word:\n",
    "            node = node.children[char]\n",
    "            prefix += char\n",
    "            if node.prefix_count == 1:\n",
    "                return prefix\n",
    "        return prefix\n",
    "\n",
    "trie = Trie()\n",
    "for string in sistrings:\n",
    "    trie.insert(string)\n",
    "\n",
    "unique_prefixes = []\n",
    "\n",
    "for string in sistrings:\n",
    "    \n",
    "    unique_prefix = trie.find_unique_prefix(string)\n",
    "    unique_prefixes.append(unique_prefix)\n",
    "    print(unique_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, char):\n",
    "        self.char = char\n",
    "        self.children = []\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def get_child(self, char):\n",
    "        for child in self.children:\n",
    "            if child.char == char:\n",
    "                return child\n",
    "        return None\n",
    "\n",
    "class PAT_Trie:\n",
    "    def __init__(self):\n",
    "        self.root = Node('')\n",
    "\n",
    "    def insert(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            child = current_node.get_child(char)\n",
    "            if child:\n",
    "                current_node = child\n",
    "            else:\n",
    "                new_node = Node(char)\n",
    "                current_node.add_child(new_node)\n",
    "                current_node = new_node\n",
    "\n",
    "    def draw(self, filename):\n",
    "        graph = pydot.Dot(graph_type='graph')\n",
    "\n",
    "        def add_node(node, parent):\n",
    "            label = node.char\n",
    "            if not label:\n",
    "                label = \"root\"\n",
    "            graph_node = pydot.Node(str(node), label=label)\n",
    "            graph.add_node(graph_node)\n",
    "            if parent is not None:\n",
    "                graph_edge = pydot.Edge(str(parent), str(node))\n",
    "                graph.add_edge(graph_edge)\n",
    "            for child in node.children:\n",
    "                add_node(child, node)\n",
    "\n",
    "        add_node(self.root, None)\n",
    "        graph.write_png(filename)\n",
    "\n",
    "# example usage\n",
    "\n",
    "\n",
    "trie = PAT_Trie()\n",
    "for s in sistrings:\n",
    "    trie.insert(s)\n",
    "\n",
    "trie.draw('pat_trie.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\togru\\ADA\\irs\\gwuxada-irs-assignments\\midterm\\temp.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     trie\u001b[39m.\u001b[39mview()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m draw_pat_trie(sistrings)\n",
      "\u001b[1;32mc:\\Users\\togru\\ADA\\irs\\gwuxada-irs-assignments\\midterm\\temp.ipynb Cell 10\u001b[0m in \u001b[0;36mdraw_pat_trie\u001b[1;34m(strings)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(s):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     prefix \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m prefix \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [edge\u001b[39m.\u001b[39mattr[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m edge \u001b[39min\u001b[39;00m trie\u001b[39m.\u001b[39;49medges(current_node)]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# Add new node and edge\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         trie\u001b[39m.\u001b[39mnode(prefix, shape\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcircle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/irs/gwuxada-irs-assignments/midterm/temp.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         trie\u001b[39m.\u001b[39medge(current_node, prefix, label\u001b[39m=\u001b[39mprefix)\n",
      "File \u001b[1;32mc:\\Users\\togru\\anaconda3\\envs\\my_env\\lib\\site-packages\\graphviz\\dot.py:242\u001b[0m, in \u001b[0;36mDot.edges\u001b[1;34m(self, tail_head_iter)\u001b[0m\n\u001b[0;32m    240\u001b[0m edge \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edge_plain\n\u001b[0;32m    241\u001b[0m quote \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quote_edge\n\u001b[1;32m--> 242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [edge(tail\u001b[39m=\u001b[39mquote(t), head\u001b[39m=\u001b[39mquote(h))\n\u001b[0;32m    243\u001b[0m               \u001b[39mfor\u001b[39;00m t, h \u001b[39min\u001b[39;00m tail_head_iter]\n",
      "File \u001b[1;32mc:\\Users\\togru\\anaconda3\\envs\\my_env\\lib\\site-packages\\graphviz\\dot.py:242\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    240\u001b[0m edge \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edge_plain\n\u001b[0;32m    241\u001b[0m quote \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quote_edge\n\u001b[1;32m--> 242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [edge(tail\u001b[39m=\u001b[39mquote(t), head\u001b[39m=\u001b[39mquote(h))\n\u001b[0;32m    243\u001b[0m               \u001b[39mfor\u001b[39;00m t, h \u001b[39min\u001b[39;00m tail_head_iter]\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def draw_pat_trie(strings):\n",
    "    # Initialize trie graph\n",
    "    trie = Digraph()\n",
    "\n",
    "    # Add root node\n",
    "    trie.node(\"root\", label=\"\", shape=\"circle\")\n",
    "\n",
    "    # Loop over strings\n",
    "    for s in strings:\n",
    "        # Add string to trie\n",
    "        current_node = \"root\"\n",
    "        prefix = \"\"\n",
    "        for i, c in enumerate(s):\n",
    "            prefix += c\n",
    "            if prefix not in [edge.attr['label'] for edge in trie.edges(current_node)]:\n",
    "                # Add new node and edge\n",
    "                trie.node(prefix, shape=\"circle\")\n",
    "                trie.edge(current_node, prefix, label=prefix)\n",
    "            current_node = prefix\n",
    "\n",
    "    # Set graph attributes\n",
    "    trie.attr(rankdir=\"LR\")\n",
    "    trie.attr(\"node\", shape=\"circle\")\n",
    "    trie.attr(\"edge\", arrowhead=\"none\")\n",
    "\n",
    "    # Render graph\n",
    "    trie.view()\n",
    "\n",
    "# Example usage\n",
    "draw_pat_trie(sistrings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self, char: str):\n",
    "        self.char = char\n",
    "        self.children = []\n",
    "        self.word_finished = False\n",
    "\n",
    "def add(root, word: str):\n",
    "    node = root\n",
    "    for char in word:\n",
    "        found_in_child = False\n",
    "        for child in node.children:\n",
    "            if child.char == char:\n",
    "                node = child\n",
    "                found_in_child = True\n",
    "                break\n",
    "        if not found_in_child:\n",
    "            new_node = TrieNode(char)\n",
    "            node.children.append(new_node)\n",
    "            node = new_node\n",
    "    node.word_finished = True\n",
    "\n",
    "def draw(parent_name, child_name):\n",
    "    edge = pydot.Edge(parent_name, child_name)\n",
    "    graph.add_edge(edge)\n",
    "\n",
    "def visit(node, parent=None):\n",
    "    for child in node.children:\n",
    "        if parent:\n",
    "            draw(parent.char, child.char)\n",
    "        visit(child, child)\n",
    "\n",
    "root = TrieNode('*')\n",
    "\n",
    "for string in sistrings:\n",
    "    add(root, string)\n",
    "\n",
    "graph = pydot.Dot(graph_type='digraph')\n",
    "visit(root)\n",
    "\n",
    "graph.write_png('pat_trie.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Probabilities if in the category fruit:\n",
      "Probability of the word lemon: (8+1) / (24+4)\n",
      "Probability of the word banana: (7+1) / (24+4)\n",
      "Probability of the word plum: (7+1) / (24+4)\n",
      "Probability of the word pear: (2+1) / (24+4)\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "corpus = [\"lemon, banana, banana, plum, plum, pear\",\n",
    "            \"pear, banana, banana, plum, banana, plum\",\n",
    "            \"lemon, banana, lemon, plum, lemon\",\n",
    "            \"lemon, banana, lemon, lemon, plum, plum, lemon\"]\n",
    "\n",
    "\n",
    "documents = [d.split(', ') for d in corpus]\n",
    "corpus_joined = \", \".join(corpus).split(', ')\n",
    "total_words = len(corpus_joined)\n",
    "\n",
    "# Calculate the probabilities of each word occurring\n",
    "word_counts = collections.Counter(corpus_joined)\n",
    "\n",
    "\n",
    "print(\">>> Probabilities if in the category fruit:\")\n",
    "\n",
    "for key, val in word_counts.items():\n",
    "    print(f'Probability of the word {key}: ({val}+1) / ({total_words}+{len(corpus)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Probabilities if in the category not fruit:\n",
      "Probability of the word banana: (5+1) / (14+4)\n",
      "Probability of the word lemon: (8+1) / (14+4)\n",
      "Probability of the word pear: (1+1) / (14+4)\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "corpus = [\"banana, lemon, pear, banana\",\n",
    "            \"lemon, banana, banana, lemon\",\n",
    "            \"banana, lemon, lemon, lemon\",\n",
    "            \"lemon, lemon\"]\n",
    "\n",
    "\n",
    "documents = [d.split(', ') for d in corpus]\n",
    "corpus_joined = \", \".join(corpus).split(', ')\n",
    "total_words = len(corpus_joined)\n",
    "\n",
    "# Calculate the probabilities of each word occurring\n",
    "word_counts = collections.Counter(corpus_joined)\n",
    "\n",
    "\n",
    "print(\">>> Probabilities if in the category not fruit:\")\n",
    "\n",
    "for key, val in word_counts.items():\n",
    "    print(f'Probability of the word {key}: ({val}+1) / ({total_words}+{len(corpus)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the word banana occurring in a document: 4 / 4\n",
      "Probability of the word lemon occurring in a document: 3 / 4\n",
      "Probability of the word pear occurring in a document: 2 / 4\n",
      "Probability of the word plum occurring in a document: 4 / 4\n"
     ]
    }
   ],
   "source": [
    "unique_in_documents = np.hstack([np.unique(d.split(', ')) for d in corpus])\n",
    "\n",
    "total_words = len(unique_in_documents)\n",
    "\n",
    "# Calculate the probabilities of each word occurring\n",
    "word_counts = collections.Counter(unique_in_documents)\n",
    "proba_occurrence_in_doc = {key: f\"{val} / {len(corpus)}\" for key, val in word_counts.items()}\n",
    "\n",
    "\n",
    "for key, val in proba_occurrence_in_doc.items():\n",
    "    print(f'Probability of the word {key} occurring in a document: {val}'   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value (IDF) of the word lemon: 1.5849625007211563\n",
      "Information value (IDF) of the word banana: 1.777607578663552\n",
      "Information value (IDF) of the word plum: 1.777607578663552\n",
      "Information value (IDF) of the word pear: 3.584962500721156\n",
      "Average information value of the whole corpus: 1.8640054628542204\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probabilities of each word occurring\n",
    "word_counts = collections.Counter(corpus_joined)\n",
    "word_proba = {key: val / total_words for key, val in word_counts.items()}\n",
    "\n",
    "\n",
    "words_idf = {key: -np.log2(val) for key, val in word_proba.items()}\n",
    "\n",
    "for key, val in words_idf.items():\n",
    "    print(f'Information value (IDF) of the word {key}: {val }')\n",
    "    \n",
    "avg_info = 0\n",
    "\n",
    "for key, val in words_idf.items():\n",
    "    avg_info += val * word_proba[key]\n",
    "    \n",
    "print(\"Average information value of the whole corpus:\", avg_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shingles(doc: str, n: int=3):\n",
    "    shingles = doc.replace(\"W\", \"\").split()\n",
    "    comb_s = [int(\"\".join(shingles[i: i+n])) for i in range(len(shingles)-(n-1))]\n",
    "    return comb_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>363</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>636</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>789</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>473</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>732</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>321</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>219</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc1  doc2\n",
       "0    363   473\n",
       "1    636   736\n",
       "2    367   367\n",
       "3    678   678\n",
       "4    789   789\n",
       "5    891   891\n",
       "6    914   914\n",
       "7    147   147\n",
       "8    473   475\n",
       "9    732   753\n",
       "10   321   538\n",
       "11   219   389"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"W3\tW6\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW3\tW2\tW1\tW9\"\n",
    "doc2 = \"W4\tW7\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW5\tW3\tW8\tW9\"\n",
    "\n",
    "comb_s1 = generate_shingles(doc1)\n",
    "comb_s2 = generate_shingles(doc2)\n",
    "\n",
    "data = np.array([comb_s1, comb_s2]).T\n",
    "df = pd.DataFrame(data, columns=[\"doc1\", \"doc2\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 / 17'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_similarity(doc1, doc2, num_lowest_shingles: int=None):\n",
    "    \n",
    "    if num_lowest_shingles:\n",
    "        comb_s2 = generate_shingles(doc2)[:num_lowest_shingles]\n",
    "        comb_s1 = generate_shingles(doc1)[:num_lowest_shingles]\n",
    "    else:\n",
    "        comb_s2 = generate_shingles(doc2)\n",
    "        comb_s1 = generate_shingles(doc1)\n",
    "        \n",
    "    return f\"{len(set(comb_s1).intersection(set(comb_s2)))} / {len(set(comb_s1).union(set(comb_s2)))}\"\n",
    "\n",
    "doc1 = \"W3\tW6\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW3\tW2\tW1\tW9\"\n",
    "doc2 = \"W4\tW7\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW5\tW3\tW8\tW9\"\n",
    "\n",
    "\n",
    "resemblence = calculate_similarity(doc1, doc2)\n",
    "resemblence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>367</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc1  doc2\n",
       "0   147   147\n",
       "1   219   367\n",
       "2   321   389\n",
       "3   363   473\n",
       "4   367   475"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"W3\tW6\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW3\tW2\tW1\tW9\"\n",
    "doc2 = \"W4\tW7\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW5\tW3\tW8\tW9\"\n",
    "\n",
    "comb_s1 = sorted(generate_shingles(doc1))[:5]\n",
    "comb_s2 = sorted(generate_shingles(doc2))[:5]\n",
    "\n",
    "data = np.array([comb_s1, comb_s2]).T\n",
    "df = pd.DataFrame(data, columns=[\"doc1\", \"doc2\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(doc1, doc2, num_lowest_shingles: int=None):\n",
    "    \n",
    "    if num_lowest_shingles:\n",
    "        comb_s2 = sorted(generate_shingles(doc2))[:num_lowest_shingles]\n",
    "        comb_s1 = sorted(generate_shingles(doc1))[:num_lowest_shingles]\n",
    "    else:\n",
    "        comb_s2 = generate_shingles(doc2)\n",
    "        comb_s1 = generate_shingles(doc1)\n",
    "        \n",
    "    return f\"{len(set(comb_s1).intersection(set(comb_s2)))} / {len(set(comb_s1).union(set(comb_s2)))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 / 8'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"W3\tW6\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW3\tW2\tW1\tW9\"\n",
    "doc2 = \"W4\tW7\tW3\tW6\tW7\tW8\tW9\tW1\tW4\tW7\tW5\tW3\tW8\tW9\"\n",
    "\n",
    "resemblence = calculate_similarity(doc1, doc2, num_lowest_shingles=5)\n",
    "resemblence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
